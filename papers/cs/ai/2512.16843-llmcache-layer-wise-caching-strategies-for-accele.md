---
title: "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference"
arxiv_id: "2512.16843"
authors: ["Harsh Vardhan Bansal"]
publication_date: 2025-12-18
field: "cs-ai"
tags: ["cs.CL", "cs.AI"]
url: "https://arxiv.org/abs/2512.16843"
pdf: "https://arxiv.org/pdf/2512.16843.pdf"
---

# LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference

## Metadata

- **arXiv ID**: [2512.16843](https://arxiv.org/abs/2512.16843)
- **Authors**: Harsh Vardhan Bansal
- **Published**: 2025-12-18
- **Collected**: 2025-12-20
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2512.16843.pdf)

## Abstract

Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CL` `cs.AI`

---

*Processed by automation system on 2025-12-20 16:29:31*
