---
title: "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent"
arxiv_id: "2512.20586"
authors: ["Humza Nusrat", "Luke Francisco", "Bing Luo", "Hassan Bagher-Ebadian", "Joshua Kim", "Karen Chin-Snyder", "Salim Siddiqui", "Mira Shah", "Eric Mellon", "Mohammad Ghassemi", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"]
publication_date: 2025-12-23
field: "cs-ai"
tags: ["cs.AI", "cs.CL", "cs.HC"]
url: "https://arxiv.org/abs/2512.20586"
pdf: "https://arxiv.org/pdf/2512.20586.pdf"
---

# Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent

## Metadata

- **arXiv ID**: [2512.20586](https://arxiv.org/abs/2512.20586)
- **Authors**: Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim, Karen Chin-Snyder, Salim Siddiqui, Mira Shah, Eric Mellon, Mohammad Ghassemi, Anthony Doemer, Benjamin Movsas, Kundan Thind
- **Published**: 2025-12-23
- **Collected**: 2025-12-24
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2512.20586.pdf)

## Abstract

Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.AI` `cs.CL` `cs.HC`

---

*Processed by automation system on 2025-12-24 08:36:00*
