---
title: "Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing"
arxiv_id: "2512.23684"
authors: ["Panagiotis Theocharopoulos", "Ajinkya Kulkarni", "Mathew Magimai. -Doss"]
publication_date: 2025-12-29
field: "cs-ai"
tags: ["cs.CL", "cs.AI"]
url: "https://arxiv.org/abs/2512.23684"
pdf: "https://arxiv.org/pdf/2512.23684.pdf"
---

# Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing

## Metadata

- **arXiv ID**: [2512.23684](https://arxiv.org/abs/2512.23684)
- **Authors**: Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai. -Doss
- **Published**: 2025-12-29
- **Collected**: 2025-12-30
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2512.23684.pdf)

## Abstract

Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CL` `cs.AI`

---

*Processed by automation system on 2025-12-30 08:35:54*
