---
title: "Parallel Token Prediction for Language Models"
arxiv_id: "2512.21323"
authors: ["Felix Draxler", "Justus Will", "Farrin Marouf Sofian", "Theofanis Karaletsos", "Sameer Singh", "Stephan Mandt"]
publication_date: 2025-12-24
field: "cs-ai"
tags: ["cs.CL", "cs.LG"]
url: "https://arxiv.org/abs/2512.21323"
pdf: "https://arxiv.org/pdf/2512.21323.pdf"
---

# Parallel Token Prediction for Language Models

## Metadata

- **arXiv ID**: [2512.21323](https://arxiv.org/abs/2512.21323)
- **Authors**: Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt
- **Published**: 2025-12-24
- **Collected**: 2025-12-25
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2512.21323.pdf)

## Abstract

We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CL` `cs.LG`

---

*Processed by automation system on 2025-12-25 16:32:13*
