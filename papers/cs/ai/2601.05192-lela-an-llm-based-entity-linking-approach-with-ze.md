---
title: "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation"
arxiv_id: "2601.05192"
authors: ["Samy Haffoudhi", "Fabian M. Suchanek", "Nils Holzenberger"]
publication_date: 2026-01-08
field: "cs-ai"
tags: ["cs.CL"]
url: "https://arxiv.org/abs/2601.05192"
pdf: "https://arxiv.org/pdf/2601.05192.pdf"
---

# LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation

## Metadata

- **arXiv ID**: [2601.05192](https://arxiv.org/abs/2601.05192)
- **Authors**: Samy Haffoudhi, Fabian M. Suchanek, Nils Holzenberger
- **Published**: 2026-01-08
- **Collected**: 2026-01-09
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2601.05192.pdf)

## Abstract

Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CL`

---

*Processed by automation system on 2026-01-09 16:35:11*
