---
title: "Internal Representations as Indicators of Hallucinations in Agent Tool Selection"
arxiv_id: "2601.05214"
authors: ["Kait Healy", "Bharathi Srinivasan", "Visakh Madathil", "Jing Wu"]
publication_date: 2026-01-08
field: "cs-ai"
tags: ["cs.AI"]
url: "https://arxiv.org/abs/2601.05214"
pdf: "https://arxiv.org/pdf/2601.05214.pdf"
---

# Internal Representations as Indicators of Hallucinations in Agent Tool Selection

## Metadata

- **arXiv ID**: [2601.05214](https://arxiv.org/abs/2601.05214)
- **Authors**: Kait Healy, Bharathi Srinivasan, Visakh Madathil, Jing Wu
- **Published**: 2026-01-08
- **Collected**: 2026-01-09
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2601.05214.pdf)

## Abstract

Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.AI`

---

*Processed by automation system on 2026-01-09 02:06:47*
