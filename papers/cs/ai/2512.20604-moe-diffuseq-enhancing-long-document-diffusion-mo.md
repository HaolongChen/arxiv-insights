---
title: "MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts"
arxiv_id: "2512.20604"
authors: ["Alexandros Christoforos", "Chadbourne Davis"]
publication_date: 2025-12-23
field: "cs-ai"
tags: ["cs.CL"]
url: "https://arxiv.org/abs/2512.20604"
pdf: "https://arxiv.org/pdf/2512.20604.pdf"
---

# MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts

## Metadata

- **arXiv ID**: [2512.20604](https://arxiv.org/abs/2512.20604)
- **Authors**: Alexandros Christoforos, Chadbourne Davis
- **Published**: 2025-12-23
- **Collected**: 2025-12-24
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2512.20604.pdf)

## Abstract

We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CL`

---

*Processed by automation system on 2025-12-24 08:36:00*
