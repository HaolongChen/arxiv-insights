---
title: "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs"
arxiv_id: "2512.20595"
authors: ["Dhruv Anand", "Ehsan Shareghi"]
publication_date: 2025-12-23
field: "cs-ai"
tags: ["cs.CL", "cs.AI", "cs.CV"]
url: "https://arxiv.org/abs/2512.20595"
pdf: "https://arxiv.org/pdf/2512.20595.pdf"
---

# Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs

## Metadata

- **arXiv ID**: [2512.20595](https://arxiv.org/abs/2512.20595)
- **Authors**: Dhruv Anand, Ehsan Shareghi
- **Published**: 2025-12-23
- **Collected**: 2025-12-24
- **Field**: CS-AI
- **PDF**: [Download](https://arxiv.org/pdf/2512.20595.pdf)

## Abstract

We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CL` `cs.AI` `cs.CV`

---

*Processed by automation system on 2025-12-24 08:36:00*
