---
title: "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs"
arxiv_id: "2512.14698"
authors: ["Jun Zhang", "Teng Wang", "Yuying Ge", "Yixiao Ge", "Xinhao Li", "Ying Shan", "Limin Wang"]
publication_date: 2025-12-16
field: "cs-cv"
tags: ["cs.CV", "cs.AI", "cs.CL", "cs.MM"]
url: "https://arxiv.org/abs/2512.14698"
pdf: "https://arxiv.org/pdf/2512.14698.pdf"
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

## Metadata

- **arXiv ID**: [2512.14698](https://arxiv.org/abs/2512.14698)
- **Authors**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang
- **Published**: 2025-12-16
- **Collected**: 2025-12-17
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.14698.pdf)

## Abstract

This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV` `cs.AI` `cs.CL` `cs.MM`

---

*Processed by automation system on 2025-12-17 08:36:56*
