---
title: "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning"
arxiv_id: "2512.12756"
authors: ["Yue Jiang", "Dingkang Yang", "Minghao Han", "Jinghang Han", "Zizhi Chen", "Yizhou Liu", "Mingcheng Li", "Peng Zhai", "Lihua Zhang"]
publication_date: 2025-12-14
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2512.12756"
pdf: "https://arxiv.org/pdf/2512.12756.pdf"
---

# FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning

## Metadata

- **arXiv ID**: [2512.12756](https://arxiv.org/abs/2512.12756)
- **Authors**: Yue Jiang, Dingkang Yang, Minghao Han, Jinghang Han, Zizhi Chen, Yizhou Liu, Mingcheng Li, Peng Zhai, Lihua Zhang
- **Published**: 2025-12-14
- **Collected**: 2025-12-16
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.12756.pdf)

## Abstract

Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2025-12-16 02:02:28*
