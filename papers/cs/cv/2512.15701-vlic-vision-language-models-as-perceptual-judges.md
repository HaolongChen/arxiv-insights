---
title: "VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression"
arxiv_id: "2512.15701"
authors: ["Kyle Sargent", "Ruiqi Gao", "Philipp Henzler", "Charles Herrmann", "Aleksander Holynski", "Li Fei-Fei", "Jiajun Wu", "Jason Zhang"]
publication_date: 2025-12-17
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2512.15701"
pdf: "https://arxiv.org/pdf/2512.15701.pdf"
---

# VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression

## Metadata

- **arXiv ID**: [2512.15701](https://arxiv.org/abs/2512.15701)
- **Authors**: Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski, Li Fei-Fei, Jiajun Wu, Jason Zhang
- **Published**: 2025-12-17
- **Collected**: 2025-12-18
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.15701.pdf)

## Abstract

Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2025-12-18 16:36:52*
