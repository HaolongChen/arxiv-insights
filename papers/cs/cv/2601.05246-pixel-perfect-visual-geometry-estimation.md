---
title: "Pixel-Perfect Visual Geometry Estimation"
arxiv_id: "2601.05246"
authors: ["Gangwei Xu", "Haotong Lin", "Hongcheng Luo", "Haiyang Sun", "Bing Wang", "Guang Chen", "Sida Peng", "Hangjun Ye", "Xin Yang"]
publication_date: 2026-01-08
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2601.05246"
pdf: "https://arxiv.org/pdf/2601.05246.pdf"
---

# Pixel-Perfect Visual Geometry Estimation

## Metadata

- **arXiv ID**: [2601.05246](https://arxiv.org/abs/2601.05246)
- **Authors**: Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang
- **Published**: 2026-01-08
- **Collected**: 2026-01-09
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2601.05246.pdf)

## Abstract

Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2026-01-09 08:37:39*
