---
title: "GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation"
arxiv_id: "2512.21276"
authors: ["Snehal Singh Tomar", "Alexandros Graikos", "Arjun Krishna", "Dimitris Samaras", "Klaus Mueller"]
publication_date: 2025-12-24
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2512.21276"
pdf: "https://arxiv.org/pdf/2512.21276.pdf"
---

# GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation

## Metadata

- **arXiv ID**: [2512.21276](https://arxiv.org/abs/2512.21276)
- **Authors**: Snehal Singh Tomar, Alexandros Graikos, Arjun Krishna, Dimitris Samaras, Klaus Mueller
- **Published**: 2025-12-24
- **Collected**: 2025-12-26
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.21276.pdf)

## Abstract

Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2025-12-26 02:02:06*
