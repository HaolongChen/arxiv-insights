---
title: "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"
arxiv_id: "2512.23646"
authors: ["Keda Tao", "Wenjie Du", "Bohan Yu", "Weiqiang Wang", "Jian Liu", "Huan Wang"]
publication_date: 2025-12-29
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2512.23646"
pdf: "https://arxiv.org/pdf/2512.23646.pdf"
---

# OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding

## Metadata

- **arXiv ID**: [2512.23646](https://arxiv.org/abs/2512.23646)
- **Authors**: Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang
- **Published**: 2025-12-29
- **Collected**: 2025-12-30
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.23646.pdf)

## Abstract

Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2025-12-30 16:34:28*
