---
title: "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering"
arxiv_id: "2601.05159"
authors: ["Shuliang Liu", "Songbo Yang", "Dong Fang", "Sihang Jia", "Yuqi Tang", "Lingfeng Su", "Ruoshui Peng", "Yibo Yan", "Xin Zou", "Xuming Hu"]
publication_date: 2026-01-08
field: "cs-cv"
tags: ["cs.CV", "cs.AI"]
url: "https://arxiv.org/abs/2601.05159"
pdf: "https://arxiv.org/pdf/2601.05159.pdf"
---

# Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering

## Metadata

- **arXiv ID**: [2601.05159](https://arxiv.org/abs/2601.05159)
- **Authors**: Shuliang Liu, Songbo Yang, Dong Fang, Sihang Jia, Yuqi Tang, Lingfeng Su, Ruoshui Peng, Yibo Yan, Xin Zou, Xuming Hu
- **Published**: 2026-01-08
- **Collected**: 2026-01-10
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2601.05159.pdf)

## Abstract

Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV` `cs.AI`

---

*Processed by automation system on 2026-01-10 16:30:57*
