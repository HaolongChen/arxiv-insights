---
title: "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation"
arxiv_id: "2512.16891"
authors: ["Haichao Zhang", "Yao Lu", "Lichen Wang", "Yunzhe Li", "Daiwei Chen", "Yunpeng Xu", "Yun Fu"]
publication_date: 2025-12-18
field: "cs-cv"
tags: ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"]
url: "https://arxiv.org/abs/2512.16891"
pdf: "https://arxiv.org/pdf/2512.16891.pdf"
---

# LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation

## Metadata

- **arXiv ID**: [2512.16891](https://arxiv.org/abs/2512.16891)
- **Authors**: Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu
- **Published**: 2025-12-18
- **Collected**: 2025-12-20
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.16891.pdf)

## Abstract

Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV` `cs.AI` `cs.IR` `cs.LG` `cs.MM`

---

*Processed by automation system on 2025-12-20 01:55:16*
