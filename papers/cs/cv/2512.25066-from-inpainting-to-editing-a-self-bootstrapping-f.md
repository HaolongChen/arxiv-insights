---
title: "From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing"
arxiv_id: "2512.25066"
authors: ["Xu He", "Haoxian Zhang", "Hejia Chen", "Changyuan Zheng", "Liyang Chen", "Songlin Tang", "Jiehui Huang", "Xiaoqiang Liu", "Pengfei Wan", "Zhiyong Wu"]
publication_date: 2025-12-31
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2512.25066"
pdf: "https://arxiv.org/pdf/2512.25066.pdf"
---

# From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing

## Metadata

- **arXiv ID**: [2512.25066](https://arxiv.org/abs/2512.25066)
- **Authors**: Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu
- **Published**: 2025-12-31
- **Collected**: 2026-01-01
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.25066.pdf)

## Abstract

Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2026-01-01 08:35:09*
