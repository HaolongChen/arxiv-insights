---
title: "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs"
arxiv_id: "2601.01804"
authors: ["Zhengjian Kang", "Qi Chen", "Rui Liu", "Kangtong Mo", "Xingyu Zhang", "Xiaoyu Deng", "Ye Zhang"]
publication_date: 2026-01-05
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2601.01804"
pdf: "https://arxiv.org/pdf/2601.01804.pdf"
---

# Causality-Aware Temporal Projection for Video Understanding in Video-LLMs

## Metadata

- **arXiv ID**: [2601.01804](https://arxiv.org/abs/2601.01804)
- **Authors**: Zhengjian Kang, Qi Chen, Rui Liu, Kangtong Mo, Xingyu Zhang, Xiaoyu Deng, Ye Zhang
- **Published**: 2026-01-05
- **Collected**: 2026-01-06
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2601.01804.pdf)

## Abstract

Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2026-01-06 02:05:28*
