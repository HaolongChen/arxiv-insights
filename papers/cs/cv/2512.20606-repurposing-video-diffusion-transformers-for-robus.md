---
title: "Repurposing Video Diffusion Transformers for Robust Point Tracking"
arxiv_id: "2512.20606"
authors: ["Soowon Son", "Honggyu An", "Chaehyun Kim", "Hyunah Ko", "Jisu Nam", "Dahyun Chung", "Siyoon Jin", "Jung Yi", "Jaewon Min", "Junhwa Hur", "Seungryong Kim"]
publication_date: 2025-12-23
field: "cs-cv"
tags: ["cs.CV"]
url: "https://arxiv.org/abs/2512.20606"
pdf: "https://arxiv.org/pdf/2512.20606.pdf"
---

# Repurposing Video Diffusion Transformers for Robust Point Tracking

## Metadata

- **arXiv ID**: [2512.20606](https://arxiv.org/abs/2512.20606)
- **Authors**: Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim
- **Published**: 2025-12-23
- **Collected**: 2025-12-24
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2512.20606.pdf)

## Abstract

Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV`

---

*Processed by automation system on 2025-12-24 02:00:45*
