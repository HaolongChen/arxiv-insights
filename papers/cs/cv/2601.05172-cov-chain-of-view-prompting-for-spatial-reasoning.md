---
title: "CoV: Chain-of-View Prompting for Spatial Reasoning"
arxiv_id: "2601.05172"
authors: ["Haoyu Zhao", "Akide Liu", "Zeyu Zhang", "Weijie Wang", "Feng Chen", "Ruihan Zhu", "Gholamreza Haffari", "Bohan Zhuang"]
publication_date: 2026-01-08
field: "cs-cv"
tags: ["cs.CV", "cs.AI"]
url: "https://arxiv.org/abs/2601.05172"
pdf: "https://arxiv.org/pdf/2601.05172.pdf"
---

# CoV: Chain-of-View Prompting for Spatial Reasoning

## Metadata

- **arXiv ID**: [2601.05172](https://arxiv.org/abs/2601.05172)
- **Authors**: Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang
- **Published**: 2026-01-08
- **Collected**: 2026-01-10
- **Field**: CS-CV
- **PDF**: [Download](https://arxiv.org/pdf/2601.05172.pdf)

## Abstract

Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CV` `cs.AI`

---

*Processed by automation system on 2026-01-10 08:32:43*
