---
title: "Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks"
arxiv_id: "2512.21315"
authors: ["Roy Turgeman", "Tom Tirer"]
publication_date: 2025-12-24
field: "cs-lg"
tags: ["cs.LG", "cs.CV", "stat.ML"]
url: "https://arxiv.org/abs/2512.21315"
pdf: "https://arxiv.org/pdf/2512.21315.pdf"
---

# Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks

## Metadata

- **arXiv ID**: [2512.21315](https://arxiv.org/abs/2512.21315)
- **Authors**: Roy Turgeman, Tom Tirer
- **Published**: 2025-12-24
- **Collected**: 2025-12-25
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.21315.pdf)

## Abstract

The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations. In particular, it suggests that there is no benefit in enhancing the signal or encoding it before addressing a classification problem. This assertion can be proven to be true for the case of the optimal Bayes classifier. However, in practice, it is common to perform "low-level" tasks before "high-level" downstream tasks despite the overwhelming capabilities of modern deep neural networks. In this paper, we aim to understand when and why low-level processing can be beneficial for classification. We present a comprehensive theoretical study of a binary classification setup, where we consider a classifier that is tightly connected to the optimal Bayes classifier and converges to it as the number of training samples increases. We prove that for any finite number of training samples, there exists a pre-classification processing that improves the classification accuracy. We also explore the effect of class separation, training set size, and class balance on the relative gain from this procedure. We support our theory with an empirical investigation of the theoretical setup. Finally, we conduct an empirical study where we investigate the effect of denoising and encoding on the performance of practical deep classifiers on benchmark datasets. Specifically, we vary the size and class distribution of the training set, and the noise level, and demonstrate trends that are consistent with our theoretical results.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.CV` `stat.ML`

---

*Processed by automation system on 2025-12-25 16:32:13*
