---
title: "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning"
arxiv_id: "2512.16911"
authors: ["Andrew Wagenmaker", "Perry Dong", "Raymond Tsao", "Chelsea Finn", "Sergey Levine"]
publication_date: 2025-12-18
field: "cs-lg"
tags: ["cs.LG", "cs.AI", "cs.RO"]
url: "https://arxiv.org/abs/2512.16911"
pdf: "https://arxiv.org/pdf/2512.16911.pdf"
---

# Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning

## Metadata

- **arXiv ID**: [2512.16911](https://arxiv.org/abs/2512.16911)
- **Authors**: Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine
- **Published**: 2025-12-18
- **Collected**: 2025-12-19
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.16911.pdf)

## Abstract

Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI` `cs.RO`

---

*Processed by automation system on 2025-12-19 16:33:35*
