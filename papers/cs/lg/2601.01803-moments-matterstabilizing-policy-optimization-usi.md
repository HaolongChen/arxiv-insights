---
title: "Moments Matter:Stabilizing Policy Optimization using Return Distributions"
arxiv_id: "2601.01803"
authors: ["Dennis Jabs", "Aditya Mohan", "Marius Lindauer"]
publication_date: 2026-01-05
field: "cs-lg"
tags: ["cs.LG", "cs.AI"]
url: "https://arxiv.org/abs/2601.01803"
pdf: "https://arxiv.org/pdf/2601.01803.pdf"
---

# Moments Matter:Stabilizing Policy Optimization using Return Distributions

## Metadata

- **arXiv ID**: [2601.01803](https://arxiv.org/abs/2601.01803)
- **Authors**: Dennis Jabs, Aditya Mohan, Marius Lindauer
- **Published**: 2026-01-05
- **Collected**: 2026-01-06
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2601.01803.pdf)

## Abstract

Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI`

---

*Processed by automation system on 2026-01-06 02:05:28*
