---
title: "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control"
arxiv_id: "2512.24955"
authors: ["Yongwei Zhang", "Yuanzhe Xing", "Quan Quan", "Zhikun She"]
publication_date: 2025-12-31
field: "cs-lg"
tags: ["cs.LG", "cs.AI", "cs.RO", "eess.SY"]
url: "https://arxiv.org/abs/2512.24955"
pdf: "https://arxiv.org/pdf/2512.24955.pdf"
---

# MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control

## Metadata

- **arXiv ID**: [2512.24955](https://arxiv.org/abs/2512.24955)
- **Authors**: Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She
- **Published**: 2025-12-31
- **Collected**: 2026-01-02
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.24955.pdf)

## Abstract

Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $Î»$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI` `cs.RO` `eess.SY`

---

*Processed by automation system on 2026-01-02 08:35:31*
