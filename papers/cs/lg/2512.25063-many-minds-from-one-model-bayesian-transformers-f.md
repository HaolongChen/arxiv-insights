---
title: "Many Minds from One Model: Bayesian Transformers for Population Intelligence"
arxiv_id: "2512.25063"
authors: ["Diji Yang", "Yi Zhang"]
publication_date: 2025-12-31
field: "cs-lg"
tags: ["cs.LG", "cs.CL"]
url: "https://arxiv.org/abs/2512.25063"
pdf: "https://arxiv.org/pdf/2512.25063.pdf"
---

# Many Minds from One Model: Bayesian Transformers for Population Intelligence

## Metadata

- **arXiv ID**: [2512.25063](https://arxiv.org/abs/2512.25063)
- **Authors**: Diji Yang, Yi Zhang
- **Published**: 2025-12-31
- **Collected**: 2026-01-01
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.25063.pdf)

## Abstract

Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.   B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.CL`

---

*Processed by automation system on 2026-01-01 08:35:09*
