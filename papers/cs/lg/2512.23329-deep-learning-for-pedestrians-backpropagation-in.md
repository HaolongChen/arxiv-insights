---
title: "Deep learning for pedestrians: backpropagation in Transformers"
arxiv_id: "2512.23329"
authors: ["Laurent Bou\u00e9"]
publication_date: 2025-12-29
field: "cs-lg"
tags: ["cs.LG"]
url: "https://arxiv.org/abs/2512.23329"
pdf: "https://arxiv.org/pdf/2512.23329.pdf"
---

# Deep learning for pedestrians: backpropagation in Transformers

## Metadata

- **arXiv ID**: [2512.23329](https://arxiv.org/abs/2512.23329)
- **Authors**: Laurent Bou√©
- **Published**: 2025-12-29
- **Collected**: 2025-12-30
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.23329.pdf)

## Abstract

This document is a follow-up to our previous paper dedicated to a vectorized derivation of backpropagation in CNNs. Following the same principles and notations already put in place there, we now focus on transformer-based next-token-prediction architectures. To this end, we apply our lightweight index-free methodology to new types of layers such as embedding, multi-headed self-attention and layer normalization. In addition, we also provide gradient expressions for LoRA layers to illustrate parameter-efficient fine-tuning. Why bother doing manual backpropagation when there are so many tools that do this automatically? Any gap in understanding of how values propagate forward will become evident when attempting to differentiate the loss function. By working through the backward pass manually, we gain a deeper intuition for how each operation influences the final output. A complete PyTorch implementation of a minimalistic GPT-like network is also provided along with analytical expressions for of all of its gradient updates.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG`

---

*Processed by automation system on 2025-12-30 02:03:39*
