---
title: "Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving"
arxiv_id: "2601.01800"
authors: ["Qi Wei", "Junchao Fan", "Zhao Yang", "Jianhua Wang", "Jingkai Mao", "Xiaolin Chang"]
publication_date: 2026-01-05
field: "cs-lg"
tags: ["cs.LG", "cs.AI"]
url: "https://arxiv.org/abs/2601.01800"
pdf: "https://arxiv.org/pdf/2601.01800.pdf"
---

# Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving

## Metadata

- **arXiv ID**: [2601.01800](https://arxiv.org/abs/2601.01800)
- **Authors**: Qi Wei, Junchao Fan, Zhao Yang, Jianhua Wang, Jingkai Mao, Xiaolin Chang
- **Published**: 2026-01-05
- **Collected**: 2026-01-06
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2601.01800.pdf)

## Abstract

Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI`

---

*Processed by automation system on 2026-01-06 02:05:28*
