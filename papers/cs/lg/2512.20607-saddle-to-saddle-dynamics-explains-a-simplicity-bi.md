---
title: "Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures"
arxiv_id: "2512.20607"
authors: ["Yedi Zhang", "Andrew Saxe", "Peter E. Latham"]
publication_date: 2025-12-23
field: "cs-lg"
tags: ["cs.LG"]
url: "https://arxiv.org/abs/2512.20607"
pdf: "https://arxiv.org/pdf/2512.20607.pdf"
---

# Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures

## Metadata

- **arXiv ID**: [2512.20607](https://arxiv.org/abs/2512.20607)
- **Authors**: Yedi Zhang, Andrew Saxe, Peter E. Latham
- **Published**: 2025-12-23
- **Collected**: 2025-12-24
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.20607.pdf)

## Abstract

Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG`

---

*Processed by automation system on 2025-12-24 02:00:45*
