---
title: "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale"
arxiv_id: "2512.10922"
authors: ["Max Zimmer", "Christophe Roux", "Moritz Wagner", "Deborah Hendrych", "Sebastian Pokutta"]
publication_date: 2025-12-11
field: "cs-lg"
tags: ["cs.LG", "cs.AI"]
url: "https://arxiv.org/abs/2512.10922"
pdf: "https://arxiv.org/pdf/2512.10922.pdf"
---

# SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale

## Metadata

- **arXiv ID**: [2512.10922](https://arxiv.org/abs/2512.10922)
- **Authors**: Max Zimmer, Christophe Roux, Moritz Wagner, Deborah Hendrych, Sebastian Pokutta
- **Published**: 2025-12-11
- **Collected**: 2025-12-13
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.10922.pdf)

## Abstract

The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI`

---

*Processed by automation system on 2025-12-13 01:55:34*
