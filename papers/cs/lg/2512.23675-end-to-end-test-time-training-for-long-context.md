---
title: "End-to-End Test-Time Training for Long Context"
arxiv_id: "2512.23675"
authors: ["Arnuv Tandon", "Karan Dalal", "Xinhao Li", "Daniel Koceja", "Marcel R\u00f8d", "Sam Buchanan", "Xiaolong Wang", "Jure Leskovec", "Sanmi Koyejo", "Tatsunori Hashimoto", "Carlos Guestrin", "Jed McCaleb", "Yejin Choi", "Yu Sun"]
publication_date: 2025-12-29
field: "cs-lg"
tags: ["cs.LG"]
url: "https://arxiv.org/abs/2512.23675"
pdf: "https://arxiv.org/pdf/2512.23675.pdf"
---

# End-to-End Test-Time Training for Long Context

## Metadata

- **arXiv ID**: [2512.23675](https://arxiv.org/abs/2512.23675)
- **Authors**: Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel RÃ¸d, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun
- **Published**: 2025-12-29
- **Collected**: 2025-12-30
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.23675.pdf)

## Abstract

We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG`

---

*Processed by automation system on 2025-12-30 16:34:28*
