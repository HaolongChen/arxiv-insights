---
title: "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs"
arxiv_id: "2512.20573"
authors: ["Rui Pan", "Zhuofu Chen", "Ravi Netravali"]
publication_date: 2025-12-23
field: "cs-lg"
tags: ["cs.LG", "cs.AI", "cs.DC"]
url: "https://arxiv.org/abs/2512.20573"
pdf: "https://arxiv.org/pdf/2512.20573.pdf"
---

# Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs

## Metadata

- **arXiv ID**: [2512.20573](https://arxiv.org/abs/2512.20573)
- **Authors**: Rui Pan, Zhuofu Chen, Ravi Netravali
- **Published**: 2025-12-23
- **Collected**: 2025-12-24
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.20573.pdf)

## Abstract

Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI` `cs.DC`

---

*Processed by automation system on 2025-12-24 16:31:46*
