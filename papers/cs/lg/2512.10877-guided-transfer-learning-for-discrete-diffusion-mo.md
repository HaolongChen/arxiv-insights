---
title: "Guided Transfer Learning for Discrete Diffusion Models"
arxiv_id: "2512.10877"
authors: ["Julian Kleutgens", "Claudio Battiloro", "Lingkai Kong", "Benjamin Grewe", "Francesca Dominici", "Mauricio Tec"]
publication_date: 2025-12-11
field: "cs-lg"
tags: ["cs.LG"]
url: "https://arxiv.org/abs/2512.10877"
pdf: "https://arxiv.org/pdf/2512.10877.pdf"
---

# Guided Transfer Learning for Discrete Diffusion Models

## Metadata

- **arXiv ID**: [2512.10877](https://arxiv.org/abs/2512.10877)
- **Authors**: Julian Kleutgens, Claudio Battiloro, Lingkai Kong, Benjamin Grewe, Francesca Dominici, Mauricio Tec
- **Published**: 2025-12-11
- **Collected**: 2025-12-13
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.10877.pdf)

## Abstract

Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG`

---

*Processed by automation system on 2025-12-13 16:30:00*
