---
title: "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization"
arxiv_id: "2512.23631"
authors: ["Iris Xu", "Guangtao Zeng", "Zexue He", "Charles Jin", "Aldo Pareja", "Dan Gutfreund", "Chuang Gan", "Zhang-Wei Hong"]
publication_date: 2025-12-29
field: "cs-lg"
tags: ["cs.LG", "cs.AI"]
url: "https://arxiv.org/abs/2512.23631"
pdf: "https://arxiv.org/pdf/2512.23631.pdf"
---

# BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization

## Metadata

- **arXiv ID**: [2512.23631](https://arxiv.org/abs/2512.23631)
- **Authors**: Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong
- **Published**: 2025-12-29
- **Collected**: 2025-12-31
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.23631.pdf)

## Abstract

Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI`

---

*Processed by automation system on 2025-12-31 02:03:57*
