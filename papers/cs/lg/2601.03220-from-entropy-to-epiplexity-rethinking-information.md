---
title: "From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence"
arxiv_id: "2601.03220"
authors: ["Marc Finzi", "Shikai Qiu", "Yiding Jiang", "Pavel Izmailov", "J. Zico Kolter", "Andrew Gordon Wilson"]
publication_date: 2026-01-06
field: "cs-lg"
tags: ["cs.LG", "stat.ML"]
url: "https://arxiv.org/abs/2601.03220"
pdf: "https://arxiv.org/pdf/2601.03220.pdf"
---

# From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence

## Metadata

- **arXiv ID**: [2601.03220](https://arxiv.org/abs/2601.03220)
- **Authors**: Marc Finzi, Shikai Qiu, Yiding Jiang, Pavel Izmailov, J. Zico Kolter, Andrew Gordon Wilson
- **Published**: 2026-01-06
- **Collected**: 2026-01-07
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2601.03220.pdf)

## Abstract

Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `stat.ML`

---

*Processed by automation system on 2026-01-07 16:37:02*
