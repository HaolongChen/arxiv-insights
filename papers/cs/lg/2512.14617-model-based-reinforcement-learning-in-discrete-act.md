---
title: "Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes"
arxiv_id: "2512.14617"
authors: ["Alessandro Trapasso", "Luca Iocchi", "Fabio Patrizi"]
publication_date: 2025-12-16
field: "cs-lg"
tags: ["cs.LG", "cs.AI"]
url: "https://arxiv.org/abs/2512.14617"
pdf: "https://arxiv.org/pdf/2512.14617.pdf"
---

# Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes

## Metadata

- **arXiv ID**: [2512.14617](https://arxiv.org/abs/2512.14617)
- **Authors**: Alessandro Trapasso, Luca Iocchi, Fabio Patrizi
- **Published**: 2025-12-16
- **Collected**: 2025-12-18
- **Field**: CS-LG
- **PDF**: [Download](https://arxiv.org/pdf/2512.14617.pdf)

## Abstract

Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.LG` `cs.AI`

---

*Processed by automation system on 2025-12-18 01:58:27*
