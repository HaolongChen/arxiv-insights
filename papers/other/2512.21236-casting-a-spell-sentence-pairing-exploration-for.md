---
title: "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking"
arxiv_id: "2512.21236"
authors: ["Yifan Huang", "Xiaojun Jia", "Wenbo Guo", "Yuqiang Sun", "Yihao Huang", "Chong Wang", "Yang Liu"]
publication_date: 2025-12-24
field: "other"
tags: ["cs.CR", "cs.AI", "cs.SE"]
url: "https://arxiv.org/abs/2512.21236"
pdf: "https://arxiv.org/pdf/2512.21236.pdf"
---

# Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking

## Metadata

- **arXiv ID**: [2512.21236](https://arxiv.org/abs/2512.21236)
- **Authors**: Yifan Huang, Xiaojun Jia, Wenbo Guo, Yuqiang Sun, Yihao Huang, Chong Wang, Yang Liu
- **Published**: 2025-12-24
- **Collected**: 2025-12-26
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2512.21236.pdf)

## Abstract

Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CR` `cs.AI` `cs.SE`

---

*Processed by automation system on 2025-12-26 08:34:31*
