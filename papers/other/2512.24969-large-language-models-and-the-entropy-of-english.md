---
title: "Large language models and the entropy of English"
arxiv_id: "2512.24969"
authors: ["Colin Scheibner", "Lindsay M. Smith", "William Bialek"]
publication_date: 2025-12-31
field: "other"
tags: ["cond-mat.stat-mech", "cs.CL", "physics.bio-ph", "q-bio.NC"]
url: "https://arxiv.org/abs/2512.24969"
pdf: "https://arxiv.org/pdf/2512.24969.pdf"
---

# Large language models and the entropy of English

## Metadata

- **arXiv ID**: [2512.24969](https://arxiv.org/abs/2512.24969)
- **Authors**: Colin Scheibner, Lindsay M. Smith, William Bialek
- **Published**: 2025-12-31
- **Collected**: 2026-01-02
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2512.24969.pdf)

## Abstract

We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cond-mat.stat-mech` `cs.CL` `physics.bio-ph` `q-bio.NC`

---

*Processed by automation system on 2026-01-02 08:35:31*
