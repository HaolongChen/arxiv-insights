---
title: "Learning Mixture Models via Efficient High-dimensional Sparse Fourier Transforms"
arxiv_id: "2601.05157"
authors: ["Alkis Kalavasis", "Pravesh K. Kothari", "Shuchen Li", "Manolis Zampetakis"]
publication_date: 2026-01-08
field: "other"
tags: ["cs.DS", "cs.LG", "stat.ML"]
url: "https://arxiv.org/abs/2601.05157"
pdf: "https://arxiv.org/pdf/2601.05157.pdf"
---

# Learning Mixture Models via Efficient High-dimensional Sparse Fourier Transforms

## Metadata

- **arXiv ID**: [2601.05157](https://arxiv.org/abs/2601.05157)
- **Authors**: Alkis Kalavasis, Pravesh K. Kothari, Shuchen Li, Manolis Zampetakis
- **Published**: 2026-01-08
- **Collected**: 2026-01-10
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2601.05157.pdf)

## Abstract

In this work, we give a ${\rm poly}(d,k)$ time and sample algorithm for efficiently learning the parameters of a mixture of $k$ spherical distributions in $d$ dimensions. Unlike all previous methods, our techniques apply to heavy-tailed distributions and include examples that do not even have finite covariances. Our method succeeds whenever the cluster distributions have a characteristic function with sufficiently heavy tails. Such distributions include the Laplace distribution but crucially exclude Gaussians.   All previous methods for learning mixture models relied implicitly or explicitly on the low-degree moments. Even for the case of Laplace distributions, we prove that any such algorithm must use super-polynomially many samples. Our method thus adds to the short list of techniques that bypass the limitations of the method of moments.   Somewhat surprisingly, our algorithm does not require any minimum separation between the cluster means. This is in stark contrast to spherical Gaussian mixtures where a minimum $\ell_2$-separation is provably necessary even information-theoretically [Regev and Vijayaraghavan '17]. Our methods compose well with existing techniques and allow obtaining ''best of both worlds" guarantees for mixtures where every component either has a heavy-tailed characteristic function or has a sub-Gaussian tail with a light-tailed characteristic function.   Our algorithm is based on a new approach to learning mixture models via efficient high-dimensional sparse Fourier transforms. We believe that this method will find more applications to statistical estimation. As an example, we give an algorithm for consistent robust mean estimation against noise-oblivious adversaries, a model practically motivated by the literature on multiple hypothesis testing. It was formally proposed in a recent Master's thesis by one of the authors, and has already inspired follow-up works.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.DS` `cs.LG` `stat.ML`

---

*Processed by automation system on 2026-01-10 16:30:57*
