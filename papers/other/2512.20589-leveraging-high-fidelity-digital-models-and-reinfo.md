---
title: "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information"
arxiv_id: "2512.20589"
authors: ["\u0130brahim O\u011fuz \u00c7etinkaya", "Sajad Khodadadian", "Taylan G. Top\u00e7u"]
publication_date: 2025-12-23
field: "other"
tags: ["cs.CY", "cs.AI", "eess.SY", "math.OC"]
url: "https://arxiv.org/abs/2512.20589"
pdf: "https://arxiv.org/pdf/2512.20589.pdf"
---

# Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information

## Metadata

- **arXiv ID**: [2512.20589](https://arxiv.org/abs/2512.20589)
- **Authors**: İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topçu
- **Published**: 2025-12-23
- **Collected**: 2025-12-24
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2512.20589.pdf)

## Abstract

As systems engineering (SE) objectives evolve from design and operation of monolithic systems to complex System of Systems (SoS), the discipline of Mission Engineering (ME) has emerged which is increasingly being accepted as a new line of thinking for the SE community. Moreover, mission environments are uncertain, dynamic, and mission outcomes are a direct function of how the mission assets will interact with this environment. This proves static architectures brittle and calls for analytically rigorous approaches for ME. To that end, this paper proposes an intelligent mission coordination methodology that integrates digital mission models with Reinforcement Learning (RL), that specifically addresses the need for adaptive task allocation and reconfiguration. More specifically, we are leveraging a Digital Engineering (DE) based infrastructure that is composed of a high-fidelity digital mission model and agent-based simulation; and then we formulate the mission tactics management problem as a Markov Decision Process (MDP), and employ an RL agent trained via Proximal Policy Optimization. By leveraging the simulation as a sandbox, we map the system states to actions, refining the policy based on realized mission outcomes. The utility of the RL-based intelligent mission coordinator is demonstrated through an aerial firefighting case study. Our findings indicate that the RL-based intelligent mission coordinator not only surpasses baseline performance but also significantly reduces the variability in mission performance. Thus, this study serves as a proof of concept demonstrating that DE-enabled mission simulations combined with advanced analytical tools offer a mission-agnostic framework for improving ME practice; which can be extended to more complicated fleet design and selection problems in the future from a mission-first perspective.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.CY` `cs.AI` `eess.SY` `math.OC`

---

*Processed by automation system on 2025-12-24 08:36:00*
