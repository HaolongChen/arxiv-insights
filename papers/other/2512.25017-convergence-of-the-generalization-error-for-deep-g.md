---
title: "Convergence of the generalization error for deep gradient flow methods for PDEs"
arxiv_id: "2512.25017"
authors: ["Chenguang Liu", "Antonis Papapantoleon", "Jasper Rou"]
publication_date: 2025-12-31
field: "other"
tags: ["math.NA", "cs.LG", "q-fin.CP", "stat.ML"]
url: "https://arxiv.org/abs/2512.25017"
pdf: "https://arxiv.org/pdf/2512.25017.pdf"
---

# Convergence of the generalization error for deep gradient flow methods for PDEs

## Metadata

- **arXiv ID**: [2512.25017](https://arxiv.org/abs/2512.25017)
- **Authors**: Chenguang Liu, Antonis Papapantoleon, Jasper Rou
- **Published**: 2025-12-31
- **Collected**: 2026-01-01
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2512.25017.pdf)

## Abstract

The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit'' and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`math.NA` `cs.LG` `q-fin.CP` `stat.ML`

---

*Processed by automation system on 2026-01-01 16:33:25*
