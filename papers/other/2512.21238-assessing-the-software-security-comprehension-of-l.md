---
title: "Assessing the Software Security Comprehension of Large Language Models"
arxiv_id: "2512.21238"
authors: ["Mohammed Latif Siddiq", "Natalie Sekerak", "Antonio Karam", "Maria Leal", "Arvin Islam-Gomes", "Joanna C. S. Santos"]
publication_date: 2025-12-24
field: "other"
tags: ["cs.SE", "cs.CR", "cs.LG"]
url: "https://arxiv.org/abs/2512.21238"
pdf: "https://arxiv.org/pdf/2512.21238.pdf"
---

# Assessing the Software Security Comprehension of Large Language Models

## Metadata

- **arXiv ID**: [2512.21238](https://arxiv.org/abs/2512.21238)
- **Authors**: Mohammed Latif Siddiq, Natalie Sekerak, Antonio Karam, Maria Leal, Arvin Islam-Gomes, Joanna C. S. Santos
- **Published**: 2025-12-24
- **Collected**: 2025-12-26
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2512.21238.pdf)

## Abstract

Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.SE` `cs.CR` `cs.LG`

---

*Processed by automation system on 2025-12-26 08:34:31*
