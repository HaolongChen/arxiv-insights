---
title: "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation"
arxiv_id: "2512.21243"
authors: ["Anatoly O. Onishchenko", "Alexey K. Kovalev", "Aleksandr I. Panov"]
publication_date: 2025-12-24
field: "other"
tags: ["cs.RO", "cs.AI", "cs.LG"]
url: "https://arxiv.org/abs/2512.21243"
pdf: "https://arxiv.org/pdf/2512.21243.pdf"
---

# LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation

## Metadata

- **arXiv ID**: [2512.21243](https://arxiv.org/abs/2512.21243)
- **Authors**: Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov
- **Published**: 2025-12-24
- **Collected**: 2025-12-26
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2512.21243.pdf)

## Abstract

Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`cs.RO` `cs.AI` `cs.LG`

---

*Processed by automation system on 2025-12-26 08:34:31*
