---
title: "Bellman Calibration for V-Learning in Offline Reinforcement Learning"
arxiv_id: "2512.23694"
authors: ["Lars van der Laan", "Nathan Kallus"]
publication_date: 2025-12-29
field: "other"
tags: ["stat.ML", "cs.LG", "econ.EM"]
url: "https://arxiv.org/abs/2512.23694"
pdf: "https://arxiv.org/pdf/2512.23694.pdf"
---

# Bellman Calibration for V-Learning in Offline Reinforcement Learning

## Metadata

- **arXiv ID**: [2512.23694](https://arxiv.org/abs/2512.23694)
- **Authors**: Lars van der Laan, Nathan Kallus
- **Published**: 2025-12-29
- **Collected**: 2025-12-30
- **Field**: OTHER
- **PDF**: [Download](https://arxiv.org/pdf/2512.23694.pdf)

## Abstract

We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.

## Key Findings

### 1. Automated extraction - requires manual review


## Methodology

See abstract and full paper for details

**Dataset**: Not specified


## Applications

- Refer to paper for specific applications


## Tags

`stat.ML` `cs.LG` `econ.EM`

---

*Processed by automation system on 2025-12-30 08:35:54*
